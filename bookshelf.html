---
title: Frederick's Bookshelf
description:
  Some books and papers that I like or need to read. This is occasionally
  updated, though not very often. I'm hoping to build out this page more
  over time. Maybe I'll add some small notes as well.
backlink: "/"
---

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    {% seo %}

    <link
      rel="preload"
      as="style"
      href="{{ site.baseurl }}/assets/css/fira_code.css"
    />
    <link rel="stylesheet" href="{{ site.baseurl }}/assets/css/fira_code.css" />

    <link
      rel="preload"
      as="style"
      href="{{ site.baseurl }}/assets/css/main.css"
    />
    <link rel="stylesheet" href="{{ site.baseurl }}/assets/css/main.css" />
  </head>

  <body>
    <div id="navigation">
      <div id="title">
        <a href="{{ page.backlink | default: '/' }}">{{ page.title }}</a>
      </div>
    </div>
    <div class="container tight">
      <div class="spacer">
        <b>Abstract:</b>
        <p>{{ page.description }}</p>
      </div>

      <h1>Papers</h1>
      <div class="bumpn">
        <h2>Papers I Like</h2>
        <ul>
          <li>
            <a
              href="https://arxiv.org/abs/2102.11174"
              class="link"
              target="_blank"
            >
              Schlag, Imanol, Kazuki Irie, and JÃ¼rgen Schmidhuber. "Linear
              transformers are secretly fast weight programmers."
              <i>International Conference on Machine Learning</i>. PMLR, 2021.
            </a>
          </li>
          <li>
            <a
              href="https://arxiv.org/abs/2212.07677"
              class="link"
              target="_blank"
            >
              Von Oswald, Johannes, et al. "Transformers learn in-context by
              gradient descent."
              <i>International Conference on Machine Learning</i>. PMLR, 2023.
            </a>
          </li>
          <li>
            <a
              href="https://arxiv.org/abs/1805.06576"
              class="link"
              target="_blank"
            >
              Balestriero, Randall, and Richard Baraniuk. "Mad max: Affine
              spline insights into deep learning."
              <i>arXiv preprint arXiv:1805.06576 (2018)</i>.
            </a>
          </li>
          <li>
            <a
              href="https://arxiv.org/abs/2302.12828"
              class="link"
              target="_blank"
            >
              Humayun, Ahmed Imtiaz, et al. "Splinecam: Exact visualization and
              characterization of deep network geometry and decision
              boundaries."
              <i>
                Proceedings of the IEEE/CVF Conference on Computer Vision and
                Pattern Recognition. 2023.
              </i>
            </a>
            <details>
              <summary>Some Notes</summary>
              <div class="note">
                This paper builds off of the above "Mad Max" paper by
                visualizing the geometry that is explored in that paper. I had
                an idea after reading "Mad Max" that motivated me to start
                implementing a similar visualization. Once I got stuck and was
                doing some research online to help my problems, I discovered
                this paper!
              </div>
            </details>
          </li>

          <li>
            <a
              href="https://arxiv.org/abs/1905.02175"
              class="link"
              target="_blank"
            >
              Ilyas, Andrew, et al. "Adversarial examples are not bugs, they are
              features."
              <i>
                Advances in neural information processing systems 32 (2019).
              </i>
            </a>
          </li>

          <li>
            <a
              href="https://arxiv.org/abs/2309.17453"
              class="link"
              target="_blank"
            >
              Xiao, Guangxuan, et al. "Efficient Streaming Language Models with
              Attention Sinks."
              <i>arXiv preprint arXiv:2309.17453</i> (2023).
            </a>
            <details>
              <summary>Some Notes</summary>
              <div class="note">
                This paper builds off of
                <a
                  href="https://www.evanmiller.org/attention-is-off-by-one.html"
                  class="link"
                  target="_blank"
                  >Evan Miller's <b>Attention is Off By One</b> blog post
                </a>
                which I think is a great (dare I say mathematically "morally
                correct") interpretation of the deficiencies of using Softmax in
                Attention. I ended up having some questions about Table 3. in
                that paper, which benchmarks Evan Miller's Softmax variant, and
                so I posted an issue
                <a
                  href="https://github.com/mit-han-lab/streaming-llm/issues/44"
                  class="link"
                  target="_blank"
                  >on the Github page for their paper</a
                >.
              </div>
            </details>
          </li>
        </ul>

        <h2>Papers I Want to Read</h2>
        <ul>
          <li>
            <a
              href="https://arxiv.org/abs/2305.10203"
              class="link"
              target="_blank"
            >
              Garnelo, Marta, and Wojciech Marian Czarnecki. "Exploring the
              Space of Key-Value-Query Models with Intention." arXiv preprint
              arXiv:2305.10203 (2023).
              <i> arXiv preprint arXiv:2305.10203 (2023).</i>
            </a>
          </li>
        </ul>
      </div>
      <br />
      <br />
      <h1>Books</h1>
      <div class="bumpn">
        <h2>Books I Like</h2>
        <p class="bumpn"></p>
        <ul>
          <li>Remembrance of Earth's Past (Trilogy) by Liu Cixin</li>
          <li>
            Calculus, Fourth Edition by Michael Spivak (ISBN-10 0914098918)
            <sup>[1]</sup>
          </li>
        </ul>

        <h2>Books I Want to Read</h2>
        <ul>
          <li>
            <i>
              A Comprehensive Introduction to Differential Geometry (Volume I)
            </i>
            by Michael Spivak
          </li>
          <li>
            <i>Principals of Mathematical Analysis</i> by Walter Rudin (AKA
            "Baby Rudin"; Reread) <sup>[2]</sup>
          </li>
          <li>
            <i>Real and Complex Analysis</i> by Walter Rudin (AKA "Papa Rudin")
            <sup>[3]</sup>
          </li>
          <li>
            <i>Functional Analysis</i> by Walter Rudin (AKA "Grandpa Rudin")
            <sup>[3]</sup>
          </li>
        </ul>
      </div>
    </div>
    <div id="footer" class="tight">
      <p>
        <sup>[1]</sup> Funnily enough, most of my favorite books I learned from
        during my undergrad. Calculus by Spivak is a great book, even to look
        back on!
        <br />
        <sup>[2]</sup> I read Baby Rudin in college and it was decent. For some
        reason I had the urge recently to make my way through the "Rudin
        family", tackling Principals, Real & Complex, and Functional Analysis.
        <br />
        <sup>[3]</sup> I'm not sure whether there is a better path through
        learning Analysis other than the Rudin family of books. I'm always open
        to suggestions for great books for learning high-level Math.
      </p>
      <p id="copyright">
        &copy; {{ site.time | date: '%Y' }} Frederick Morlock. All rights
        reserved.
      </p>
    </div>
  </body>
</html>
